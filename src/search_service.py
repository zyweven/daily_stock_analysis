# -*- coding: utf-8 -*-
"""
===================================
Aè‚¡è‡ªé€‰è‚¡æ™ºèƒ½åˆ†æç³»ç»Ÿ - æœç´¢æœåŠ¡æ¨¡å—
===================================

èŒè´£ï¼š
1. æä¾›ç»Ÿä¸€çš„æ–°é—»æœç´¢æ¥å£
2. æ”¯æŒ Tavily å’Œ SerpAPI ä¸¤ç§æœç´¢å¼•æ“
3. å¤š Key è´Ÿè½½å‡è¡¡å’Œæ•…éšœè½¬ç§»
4. æœç´¢ç»“æœç¼“å­˜å’Œæ ¼å¼åŒ–
"""

import logging
import random
import time
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from itertools import cycle
import requests
from newspaper import Article, Config

logger = logging.getLogger(__name__)


def fetch_url_content(url: str, timeout: int = 5) -> str:
    """
    è·å– URL ç½‘é¡µæ­£æ–‡å†…å®¹ (ä½¿ç”¨ newspaper3k)
    """
    try:
        # é…ç½® newspaper3k
        config = Config()
        config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        config.request_timeout = timeout
        config.fetch_images = False  # ä¸ä¸‹è½½å›¾ç‰‡
        config.memoize_articles = False # ä¸ç¼“å­˜

        article = Article(url, config=config, language='zh') # é»˜è®¤ä¸­æ–‡ï¼Œä½†ä¹Ÿæ”¯æŒå…¶ä»–
        article.download()
        article.parse()

        # è·å–æ­£æ–‡
        text = article.text.strip()

        # ç®€å•çš„åå¤„ç†ï¼Œå»é™¤ç©ºè¡Œ
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        text = '\n'.join(lines)

        return text[:1500]  # é™åˆ¶è¿”å›é•¿åº¦ï¼ˆæ¯” bs4 ç¨å¾®å¤šä¸€ç‚¹ï¼Œå› ä¸º newspaper è§£ææ›´å¹²å‡€ï¼‰
    except Exception as e:
        logger.debug(f"Fetch content failed for {url}: {e}")

    return ""


@dataclass
class SearchResult:
    """æœç´¢ç»“æœæ•°æ®ç±»"""
    title: str
    snippet: str  # æ‘˜è¦
    url: str
    source: str  # æ¥æºç½‘ç«™
    published_date: Optional[str] = None
    
    def to_text(self) -> str:
        """è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼"""
        date_str = f" ({self.published_date})" if self.published_date else ""
        return f"ã€{self.source}ã€‘{self.title}{date_str}\n{self.snippet}"


@dataclass 
class SearchResponse:
    """æœç´¢å“åº”"""
    query: str
    results: List[SearchResult]
    provider: str  # ä½¿ç”¨çš„æœç´¢å¼•æ“
    success: bool = True
    error_message: Optional[str] = None
    search_time: float = 0.0  # æœç´¢è€—æ—¶ï¼ˆç§’ï¼‰
    
    def to_context(self, max_results: int = 5) -> str:
        """å°†æœç´¢ç»“æœè½¬æ¢ä¸ºå¯ç”¨äº AI åˆ†æçš„ä¸Šä¸‹æ–‡"""
        if not self.success or not self.results:
            return f"æœç´¢ '{self.query}' æœªæ‰¾åˆ°ç›¸å…³ç»“æœã€‚"
        
        lines = [f"ã€{self.query} æœç´¢ç»“æœã€‘ï¼ˆæ¥æºï¼š{self.provider}ï¼‰"]
        for i, result in enumerate(self.results[:max_results], 1):
            lines.append(f"\n{i}. {result.to_text()}")
        
        return "\n".join(lines)


class BaseSearchProvider(ABC):
    """æœç´¢å¼•æ“åŸºç±»"""
    
    def __init__(self, api_keys: List[str], name: str):
        """
        åˆå§‹åŒ–æœç´¢å¼•æ“
        
        Args:
            api_keys: API Key åˆ—è¡¨ï¼ˆæ”¯æŒå¤šä¸ª key è´Ÿè½½å‡è¡¡ï¼‰
            name: æœç´¢å¼•æ“åç§°
        """
        self._api_keys = api_keys
        self._name = name
        self._key_cycle = cycle(api_keys) if api_keys else None
        self._key_usage: Dict[str, int] = {key: 0 for key in api_keys}
        self._key_errors: Dict[str, int] = {key: 0 for key in api_keys}
    
    @property
    def name(self) -> str:
        return self._name
    
    @property
    def is_available(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ API Key"""
        return bool(self._api_keys)
    
    def _get_next_key(self) -> Optional[str]:
        """
        è·å–ä¸‹ä¸€ä¸ªå¯ç”¨çš„ API Keyï¼ˆè´Ÿè½½å‡è¡¡ï¼‰
        
        ç­–ç•¥ï¼šè½®è¯¢ + è·³è¿‡é”™è¯¯è¿‡å¤šçš„ key
        """
        if not self._key_cycle:
            return None
        
        # æœ€å¤šå°è¯•æ‰€æœ‰ key
        for _ in range(len(self._api_keys)):
            key = next(self._key_cycle)
            # è·³è¿‡é”™è¯¯æ¬¡æ•°è¿‡å¤šçš„ keyï¼ˆè¶…è¿‡ 3 æ¬¡ï¼‰
            if self._key_errors.get(key, 0) < 3:
                return key
        
        # æ‰€æœ‰ key éƒ½æœ‰é—®é¢˜ï¼Œé‡ç½®é”™è¯¯è®¡æ•°å¹¶è¿”å›ç¬¬ä¸€ä¸ª
        logger.warning(f"[{self._name}] æ‰€æœ‰ API Key éƒ½æœ‰é”™è¯¯è®°å½•ï¼Œé‡ç½®é”™è¯¯è®¡æ•°")
        self._key_errors = {key: 0 for key in self._api_keys}
        return self._api_keys[0] if self._api_keys else None
    
    def _record_success(self, key: str) -> None:
        """è®°å½•æˆåŠŸä½¿ç”¨"""
        self._key_usage[key] = self._key_usage.get(key, 0) + 1
        # æˆåŠŸåå‡å°‘é”™è¯¯è®¡æ•°
        if key in self._key_errors and self._key_errors[key] > 0:
            self._key_errors[key] -= 1
    
    def _record_error(self, key: str) -> None:
        """è®°å½•é”™è¯¯"""
        self._key_errors[key] = self._key_errors.get(key, 0) + 1
        logger.warning(f"[{self._name}] API Key {key[:8]}... é”™è¯¯è®¡æ•°: {self._key_errors[key]}")
    
    @abstractmethod
    def _do_search(self, query: str, api_key: str, max_results: int, days: int = 7) -> SearchResponse:
        """æ‰§è¡Œæœç´¢ï¼ˆå­ç±»å®ç°ï¼‰"""
        pass
    
    def search(self, query: str, max_results: int = 5, days: int = 7) -> SearchResponse:
        """
        æ‰§è¡Œæœç´¢
        
        Args:
            query: æœç´¢å…³é”®è¯
            max_results: æœ€å¤§è¿”å›ç»“æœæ•°
            days: æœç´¢æœ€è¿‘å‡ å¤©çš„æ—¶é—´èŒƒå›´ï¼ˆé»˜è®¤7å¤©ï¼‰
            
        Returns:
            SearchResponse å¯¹è±¡
        """
        api_key = self._get_next_key()
        if not api_key:
            return SearchResponse(
                query=query,
                results=[],
                provider=self._name,
                success=False,
                error_message=f"{self._name} æœªé…ç½® API Key"
            )
        
        start_time = time.time()
        try:
            response = self._do_search(query, api_key, max_results, days=days)
            response.search_time = time.time() - start_time
            
            if response.success:
                self._record_success(api_key)
                logger.info(f"[{self._name}] æœç´¢ '{query}' æˆåŠŸï¼Œè¿”å› {len(response.results)} æ¡ç»“æœï¼Œè€—æ—¶ {response.search_time:.2f}s")
            else:
                self._record_error(api_key)
            
            return response
            
        except Exception as e:
            self._record_error(api_key)
            elapsed = time.time() - start_time
            logger.error(f"[{self._name}] æœç´¢ '{query}' å¤±è´¥: {e}")
            return SearchResponse(
                query=query,
                results=[],
                provider=self._name,
                success=False,
                error_message=str(e),
                search_time=elapsed
            )


class TavilySearchProvider(BaseSearchProvider):
    """
    Tavily æœç´¢å¼•æ“
    
    ç‰¹ç‚¹ï¼š
    - ä¸“ä¸º AI/LLM ä¼˜åŒ–çš„æœç´¢ API
    - å…è´¹ç‰ˆæ¯æœˆ 1000 æ¬¡è¯·æ±‚
    - è¿”å›ç»“æ„åŒ–çš„æœç´¢ç»“æœ
    
    æ–‡æ¡£ï¼šhttps://docs.tavily.com/
    """
    
    def __init__(self, api_keys: List[str]):
        super().__init__(api_keys, "Tavily")
    
    def _do_search(self, query: str, api_key: str, max_results: int, days: int = 7) -> SearchResponse:
        """æ‰§è¡Œ Tavily æœç´¢"""
        try:
            from tavily import TavilyClient
        except ImportError:
            return SearchResponse(
                query=query,
                results=[],
                provider=self.name,
                success=False,
                error_message="tavily-python æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install tavily-python"
            )
        
        try:
            client = TavilyClient(api_key=api_key)
            
            # æ‰§è¡Œæœç´¢ï¼ˆä¼˜åŒ–ï¼šä½¿ç”¨advancedæ·±åº¦ã€é™åˆ¶æœ€è¿‘å‡ å¤©ï¼‰
            response = client.search(
                query=query,
                search_depth="advanced",  # advanced è·å–æ›´å¤šç»“æœ
                max_results=max_results,
                include_answer=False,
                include_raw_content=False,
                days=days,  # æœç´¢æœ€è¿‘å¤©æ•°çš„å†…å®¹
            )
            
            # è®°å½•åŸå§‹å“åº”åˆ°æ—¥å¿—
            logger.info(f"[Tavily] æœç´¢å®Œæˆï¼Œquery='{query}', è¿”å› {len(response.get('results', []))} æ¡ç»“æœ")
            logger.debug(f"[Tavily] åŸå§‹å“åº”: {response}")
            
            # è§£æç»“æœ
            results = []
            for item in response.get('results', []):
                results.append(SearchResult(
                    title=item.get('title', ''),
                    snippet=item.get('content', '')[:500],  # æˆªå–å‰500å­—
                    url=item.get('url', ''),
                    source=self._extract_domain(item.get('url', '')),
                    published_date=item.get('published_date'),
                ))
            
            return SearchResponse(
                query=query,
                results=results,
                provider=self.name,
                success=True,
            )
            
        except Exception as e:
            error_msg = str(e)
            # æ£€æŸ¥æ˜¯å¦æ˜¯é…é¢é—®é¢˜
            if 'rate limit' in error_msg.lower() or 'quota' in error_msg.lower():
                error_msg = f"API é…é¢å·²ç”¨å°½: {error_msg}"
            
            return SearchResponse(
                query=query,
                results=[],
                provider=self.name,
                success=False,
                error_message=error_msg
            )
    
    @staticmethod
    def _extract_domain(url: str) -> str:
        """ä» URL æå–åŸŸåä½œä¸ºæ¥æº"""
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            domain = parsed.netloc.replace('www.', '')
            return domain or 'æœªçŸ¥æ¥æº'
        except:
            return 'æœªçŸ¥æ¥æº'


class SerpAPISearchProvider(BaseSearchProvider):
    """
    SerpAPI æœç´¢å¼•æ“
    
    ç‰¹ç‚¹ï¼š
    - æ”¯æŒ Googleã€Bingã€ç™¾åº¦ç­‰å¤šç§æœç´¢å¼•æ“
    - å…è´¹ç‰ˆæ¯æœˆ 100 æ¬¡è¯·æ±‚
    - è¿”å›çœŸå®çš„æœç´¢ç»“æœ
    
    æ–‡æ¡£ï¼šhttps://serpapi.com/baidu-search-api?utm_source=github_daily_stock_analysis
    """
    
    def __init__(self, api_keys: List[str]):
        super().__init__(api_keys, "SerpAPI")
    
    def _do_search(self, query: str, api_key: str, max_results: int, days: int = 7) -> SearchResponse:
        """æ‰§è¡Œ SerpAPI æœç´¢"""
        try:
            from serpapi import GoogleSearch
        except ImportError:
            return SearchResponse(
                query=query,
                results=[],
                provider=self.name,
                success=False,
                error_message="google-search-results æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install google-search-results"
            )
        
        try:
            # ç¡®å®šæ—¶é—´èŒƒå›´å‚æ•° tbs
            tbs = "qdr:w"  # é»˜è®¤ä¸€å‘¨
            if days <= 1:
                tbs = "qdr:d"  # è¿‡å»24å°æ—¶
            elif days <= 7:
                tbs = "qdr:w"  # è¿‡å»ä¸€å‘¨
            elif days <= 30:
                tbs = "qdr:m"  # è¿‡å»ä¸€æœˆ
            else:
                tbs = "qdr:y"  # è¿‡å»ä¸€å¹´

            # ä½¿ç”¨ Google æœç´¢ (è·å– Knowledge Graph, Answer Box ç­‰)
            params = {
                "engine": "google",
                "q": query,
                "api_key": api_key,
                "google_domain": "google.com.hk", # ä½¿ç”¨é¦™æ¸¯è°·æ­Œï¼Œä¸­æ–‡æ”¯æŒè¾ƒå¥½
                "hl": "zh-cn",  # ä¸­æ–‡ç•Œé¢
                "gl": "cn",     # ä¸­å›½åœ°åŒºåå¥½
                "tbs": tbs,     # æ—¶é—´èŒƒå›´é™åˆ¶
                "num": max_results # è¯·æ±‚çš„ç»“æœæ•°é‡ï¼Œæ³¨æ„ï¼šGoogle APIæœ‰æ—¶ä¸ä¸¥æ ¼éµå®ˆ
            }
            
            search = GoogleSearch(params)
            response = search.get_dict()
            
            # è®°å½•åŸå§‹å“åº”åˆ°æ—¥å¿—
            logger.debug(f"[SerpAPI] åŸå§‹å“åº” keys: {response.keys()}")
            
            # è§£æç»“æœ
            results = []
            
            # 1. è§£æ Knowledge Graph (çŸ¥è¯†å›¾è°±)
            kg = response.get('knowledge_graph', {})
            if kg:
                title = kg.get('title', 'çŸ¥è¯†å›¾è°±')
                desc = kg.get('description', '')
                
                # æå–é¢å¤–å±æ€§
                details = []
                for key in ['type', 'founded', 'headquarters', 'employees', 'ceo']:
                    val = kg.get(key)
                    if val:
                        details.append(f"{key}: {val}")
                        
                snippet = f"{desc}\n" + " | ".join(details) if details else desc
                
                results.append(SearchResult(
                    title=f"[çŸ¥è¯†å›¾è°±] {title}",
                    snippet=snippet,
                    url=kg.get('source', {}).get('link', ''),
                    source="Google Knowledge Graph"
                ))
                
            # 2. è§£æ Answer Box (ç²¾é€‰å›ç­”/è¡Œæƒ…å¡ç‰‡)
            ab = response.get('answer_box', {})
            if ab:
                ab_title = ab.get('title', 'ç²¾é€‰å›ç­”')
                ab_snippet = ""
                
                # è´¢ç»ç±»å›ç­”
                if ab.get('type') == 'finance_results':
                    stock = ab.get('stock', '')
                    price = ab.get('price', '')
                    currency = ab.get('currency', '')
                    movement = ab.get('price_movement', {})
                    mv_val = movement.get('percentage', 0)
                    mv_dir = movement.get('movement', '')
                    
                    ab_title = f"[è¡Œæƒ…å¡ç‰‡] {stock}"
                    ab_snippet = f"ä»·æ ¼: {price} {currency}\næ¶¨è·Œ: {mv_dir} {mv_val}%"
                    
                    # æå–è¡¨æ ¼æ•°æ®
                    if 'table' in ab:
                        table_data = []
                        for row in ab['table']:
                            if 'name' in row and 'value' in row:
                                table_data.append(f"{row['name']}: {row['value']}")
                        if table_data:
                            ab_snippet += "\n" + "; ".join(table_data)
                            
                # æ™®é€šæ–‡æœ¬å›ç­”
                elif 'snippet' in ab:
                    ab_snippet = ab.get('snippet', '')
                    list_items = ab.get('list', [])
                    if list_items:
                        ab_snippet += "\n" + "\n".join([f"- {item}" for item in list_items])
                
                elif 'answer' in ab:
                    ab_snippet = ab.get('answer', '')
                    
                if ab_snippet:
                    results.append(SearchResult(
                        title=f"[ç²¾é€‰å›ç­”] {ab_title}",
                        snippet=ab_snippet,
                        url=ab.get('link', '') or ab.get('displayed_link', ''),
                        source="Google Answer Box"
                    ))

            # 3. è§£æ Related Questions (ç›¸å…³é—®é¢˜)
            rqs = response.get('related_questions', [])
            for rq in rqs[:3]: # å–å‰3ä¸ª
                question = rq.get('question', '')
                snippet = rq.get('snippet', '')
                link = rq.get('link', '')
                
                if question and snippet:
                     results.append(SearchResult(
                        title=f"[ç›¸å…³é—®é¢˜] {question}",
                        snippet=snippet,
                        url=link,
                        source="Google Related Questions"
                     ))

            # 4. è§£æ Organic Results (è‡ªç„¶æœç´¢ç»“æœ)
            organic_results = response.get('organic_results', [])

            for item in organic_results[:max_results]:
                link = item.get('link', '')
                snippet = item.get('snippet', '')

                # å¢å¼ºï¼šå¦‚æœéœ€è¦ï¼Œè§£æç½‘é¡µæ­£æ–‡
                # ç­–ç•¥ï¼šå¦‚æœæ‘˜è¦å¤ªçŸ­ï¼Œæˆ–è€…ä¸ºäº†è·å–æ›´å¤šä¿¡æ¯ï¼Œå¯ä»¥è¯·æ±‚ç½‘é¡µ
                # è¿™é‡Œæˆ‘ä»¬å¯¹æ‰€æœ‰ç»“æœå°è¯•è·å–æ­£æ–‡ï¼Œä½†ä¸ºäº†æ€§èƒ½ï¼Œä»…è·å–å‰1000å­—ç¬¦
                content = ""
                if link:
                   try:
                       fetched_content = fetch_url_content(link, timeout=5)
                       if fetched_content:
                           # å¦‚æœè·å–åˆ°äº†æ­£æ–‡ï¼Œå°†å…¶æ‹¼æ¥åˆ° snippet ä¸­ï¼Œæˆ–è€…æ›¿æ¢ snippet
                           # è¿™é‡Œé€‰æ‹©æ‹¼æ¥ï¼Œä¿ç•™åŸæ‘˜è¦
                           content = fetched_content
                           if len(content) > 500:
                               snippet = f"{snippet}\n\nã€ç½‘é¡µè¯¦æƒ…ã€‘\n{content[:500]}..."
                           else:
                               snippet = f"{snippet}\n\nã€ç½‘é¡µè¯¦æƒ…ã€‘\n{content}"
                   except Exception as e:
                       logger.debug(f"[SerpAPI] Fetch content failed: {e}")

                results.append(SearchResult(
                    title=item.get('title', ''),
                    snippet=snippet[:1000], # é™åˆ¶æ€»é•¿åº¦
                    url=link,
                    source=item.get('source', self._extract_domain(link)),
                    published_date=item.get('date'),
                ))

            return SearchResponse(
                query=query,
                results=results,
                provider=self.name,
                success=True,
            )
            
        except Exception as e:
            error_msg = str(e)
            return SearchResponse(
                query=query,
                results=[],
                provider=self.name,
                success=False,
                error_message=error_msg
            )
    
    @staticmethod
    def _extract_domain(url: str) -> str:
        """ä» URL æå–åŸŸå"""
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            return parsed.netloc.replace('www.', '') or 'æœªçŸ¥æ¥æº'
        except:
            return 'æœªçŸ¥æ¥æº'


class BochaSearchProvider(BaseSearchProvider):
    """
    åšæŸ¥æœç´¢å¼•æ“
    
    ç‰¹ç‚¹ï¼š
    - ä¸“ä¸ºAIä¼˜åŒ–çš„ä¸­æ–‡æœç´¢API
    - ç»“æœå‡†ç¡®ã€æ‘˜è¦å®Œæ•´
    - æ”¯æŒæ—¶é—´èŒƒå›´è¿‡æ»¤å’ŒAIæ‘˜è¦
    - å…¼å®¹Bing Search APIæ ¼å¼
    
    æ–‡æ¡£ï¼šhttps://bocha-ai.feishu.cn/wiki/RXEOw02rFiwzGSkd9mUcqoeAnNK
    """
    
    def __init__(self, api_keys: List[str]):
        super().__init__(api_keys, "Bocha")
    
    def _do_search(self, query: str, api_key: str, max_results: int, days: int = 7) -> SearchResponse:
        """æ‰§è¡ŒåšæŸ¥æœç´¢"""
        try:
            import requests
        except ImportError:
            return SearchResponse(
                query=query,
                results=[],
                provider=self.name,
                success=False,
                error_message="requests æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install requests"
            )
        
        try:
            # API ç«¯ç‚¹
            url = "https://api.bocha.cn/v1/web-search"
            
            # è¯·æ±‚å¤´
            headers = {
                'Authorization': f'Bearer {api_key}',
                'Content-Type': 'application/json'
            }
            
            # ç¡®å®šæ—¶é—´èŒƒå›´
            freshness = "oneWeek"
            if days <= 1:
                freshness = "oneDay"
            elif days <= 7:
                freshness = "oneWeek"
            elif days <= 30:
                freshness = "oneMonth"
            else:
                freshness = "oneYear"

            # è¯·æ±‚å‚æ•°ï¼ˆä¸¥æ ¼æŒ‰ç…§APIæ–‡æ¡£ï¼‰
            payload = {
                "query": query,
                "freshness": freshness,  # åŠ¨æ€æ—¶é—´èŒƒå›´
                "summary": True,  # å¯ç”¨AIæ‘˜è¦
                "count": min(max_results, 50)  # æœ€å¤§50æ¡
            }
            
            # æ‰§è¡Œæœç´¢
            response = requests.post(url, headers=headers, json=payload, timeout=10)
            
            # æ£€æŸ¥HTTPçŠ¶æ€ç 
            if response.status_code != 200:
                # å°è¯•è§£æé”™è¯¯ä¿¡æ¯
                try:
                    if response.headers.get('content-type', '').startswith('application/json'):
                        error_data = response.json()
                        error_message = error_data.get('message', response.text)
                    else:
                        error_message = response.text
                except:
                    error_message = response.text
                
                # æ ¹æ®é”™è¯¯ç å¤„ç†
                if response.status_code == 403:
                    error_msg = f"ä½™é¢ä¸è¶³: {error_message}"
                elif response.status_code == 401:
                    error_msg = f"API KEYæ— æ•ˆ: {error_message}"
                elif response.status_code == 400:
                    error_msg = f"è¯·æ±‚å‚æ•°é”™è¯¯: {error_message}"
                elif response.status_code == 429:
                    error_msg = f"è¯·æ±‚é¢‘ç‡è¾¾åˆ°é™åˆ¶: {error_message}"
                else:
                    error_msg = f"HTTP {response.status_code}: {error_message}"
                
                logger.warning(f"[Bocha] æœç´¢å¤±è´¥: {error_msg}")
                
                return SearchResponse(
                    query=query,
                    results=[],
                    provider=self.name,
                    success=False,
                    error_message=error_msg
                )
            
            # è§£æå“åº”
            try:
                data = response.json()
            except ValueError as e:
                error_msg = f"å“åº”JSONè§£æå¤±è´¥: {str(e)}"
                logger.error(f"[Bocha] {error_msg}")
                return SearchResponse(
                    query=query,
                    results=[],
                    provider=self.name,
                    success=False,
                    error_message=error_msg
                )
            
            # æ£€æŸ¥å“åº”code
            if data.get('code') != 200:
                error_msg = data.get('msg') or f"APIè¿”å›é”™è¯¯ç : {data.get('code')}"
                return SearchResponse(
                    query=query,
                    results=[],
                    provider=self.name,
                    success=False,
                    error_message=error_msg
                )
            
            # è®°å½•åŸå§‹å“åº”åˆ°æ—¥å¿—
            logger.info(f"[Bocha] æœç´¢å®Œæˆï¼Œquery='{query}'")
            logger.debug(f"[Bocha] åŸå§‹å“åº”: {data}")
            
            # è§£ææœç´¢ç»“æœ
            results = []
            web_pages = data.get('data', {}).get('webPages', {})
            value_list = web_pages.get('value', [])
            
            for item in value_list[:max_results]:
                # ä¼˜å…ˆä½¿ç”¨summaryï¼ˆAIæ‘˜è¦ï¼‰ï¼Œfallbackåˆ°snippet
                snippet = item.get('summary') or item.get('snippet', '')
                
                # æˆªå–æ‘˜è¦é•¿åº¦
                if snippet:
                    snippet = snippet[:500]
                
                results.append(SearchResult(
                    title=item.get('name', ''),
                    snippet=snippet,
                    url=item.get('url', ''),
                    source=item.get('siteName') or self._extract_domain(item.get('url', '')),
                    published_date=item.get('datePublished'),  # UTC+8æ ¼å¼ï¼Œæ— éœ€è½¬æ¢
                ))
            
            logger.info(f"[Bocha] æˆåŠŸè§£æ {len(results)} æ¡ç»“æœ")
            
            return SearchResponse(
                query=query,
                results=results,
                provider=self.name,
                success=True,
            )
            
        except requests.exceptions.Timeout:
            error_msg = "è¯·æ±‚è¶…æ—¶"
            logger.error(f"[Bocha] {error_msg}")
            return SearchResponse(
                query=query,
                results=[],
                provider=self.name,
                success=False,
                error_message=error_msg
            )
        except requests.exceptions.RequestException as e:
            error_msg = f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"
            logger.error(f"[Bocha] {error_msg}")
            return SearchResponse(
                query=query,
                results=[],
                provider=self.name,
                success=False,
                error_message=error_msg
            )
        except Exception as e:
            error_msg = f"æœªçŸ¥é”™è¯¯: {str(e)}"
            logger.error(f"[Bocha] {error_msg}")
            return SearchResponse(
                query=query,
                results=[],
                provider=self.name,
                success=False,
                error_message=error_msg
            )
    
    @staticmethod
    def _extract_domain(url: str) -> str:
        """ä» URL æå–åŸŸåä½œä¸ºæ¥æº"""
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            domain = parsed.netloc.replace('www.', '')
            return domain or 'æœªçŸ¥æ¥æº'
        except:
            return 'æœªçŸ¥æ¥æº'


class BraveSearchProvider(BaseSearchProvider):
    """
    Brave Search æœç´¢å¼•æ“

    ç‰¹ç‚¹ï¼š
    - éšç§ä¼˜å…ˆçš„ç‹¬ç«‹æœç´¢å¼•æ“
    - ç´¢å¼•è¶…è¿‡300äº¿é¡µé¢
    - å…è´¹å±‚å¯ç”¨
    - æ”¯æŒæ—¶é—´èŒƒå›´è¿‡æ»¤

    æ–‡æ¡£ï¼šhttps://brave.com/search/api/
    """

    API_ENDPOINT = "https://api.search.brave.com/res/v1/web/search"

    def __init__(self, api_keys: List[str]):
        super().__init__(api_keys, "Brave")

    def _do_search(self, query: str, api_key: str, max_results: int, days: int = 7) -> SearchResponse:
        """æ‰§è¡Œ Brave æœç´¢"""
        try:
            # è¯·æ±‚å¤´
            headers = {
                'X-Subscription-Token': api_key,
                'Accept': 'application/json'
            }

            # ç¡®å®šæ—¶é—´èŒƒå›´ï¼ˆfreshness å‚æ•°ï¼‰
            if days <= 1:
                freshness = "pd"  # Past day (24å°æ—¶)
            elif days <= 7:
                freshness = "pw"  # Past week
            elif days <= 30:
                freshness = "pm"  # Past month
            else:
                freshness = "py"  # Past year

            # è¯·æ±‚å‚æ•°
            params = {
                "q": query,
                "count": min(max_results, 20),  # Brave æœ€å¤§æ”¯æŒ20æ¡
                "freshness": freshness,
                "search_lang": "en",  # è‹±æ–‡å†…å®¹ï¼ˆUSè‚¡ç¥¨ä¼˜å…ˆï¼‰
                "country": "US",  # ç¾å›½åŒºåŸŸåå¥½
                "safesearch": "moderate"
            }

            # æ‰§è¡Œæœç´¢ï¼ˆGET è¯·æ±‚ï¼‰
            response = requests.get(
                self.API_ENDPOINT,
                headers=headers,
                params=params,
                timeout=10
            )

            # æ£€æŸ¥HTTPçŠ¶æ€ç 
            if response.status_code != 200:
                error_msg = self._parse_error(response)
                logger.warning(f"[Brave] æœç´¢å¤±è´¥: {error_msg}")
                return SearchResponse(
                    query=query,
                    results=[],
                    provider=self.name,
                    success=False,
                    error_message=error_msg
                )

            # è§£æå“åº”
            try:
                data = response.json()
            except ValueError as e:
                error_msg = f"å“åº”JSONè§£æå¤±è´¥: {str(e)}"
                logger.error(f"[Brave] {error_msg}")
                return SearchResponse(
                    query=query,
                    results=[],
                    provider=self.name,
                    success=False,
                    error_message=error_msg
                )

            logger.info(f"[Brave] æœç´¢å®Œæˆï¼Œquery='{query}'")
            logger.debug(f"[Brave] åŸå§‹å“åº”: {data}")

            # è§£ææœç´¢ç»“æœ
            results = []
            web_data = data.get('web', {})
            web_results = web_data.get('results', [])

            for item in web_results[:max_results]:
                # è§£æå‘å¸ƒæ—¥æœŸï¼ˆISO 8601 æ ¼å¼ï¼‰
                published_date = None
                age = item.get('age') or item.get('page_age')
                if age:
                    try:
                        # è½¬æ¢ ISO æ ¼å¼ä¸ºç®€å•æ—¥æœŸå­—ç¬¦ä¸²
                        dt = datetime.fromisoformat(age.replace('Z', '+00:00'))
                        published_date = dt.strftime('%Y-%m-%d')
                    except (ValueError, AttributeError):
                        published_date = age  # è§£æå¤±è´¥æ—¶ä½¿ç”¨åŸå§‹å€¼

                results.append(SearchResult(
                    title=item.get('title', ''),
                    snippet=item.get('description', '')[:500],  # æˆªå–åˆ°500å­—ç¬¦
                    url=item.get('url', ''),
                    source=self._extract_domain(item.get('url', '')),
                    published_date=published_date
                ))

            logger.info(f"[Brave] æˆåŠŸè§£æ {len(results)} æ¡ç»“æœ")

            return SearchResponse(
                query=query,
                results=results,
                provider=self.name,
                success=True
            )

        except requests.exceptions.Timeout:
            error_msg = "è¯·æ±‚è¶…æ—¶"
            logger.error(f"[Brave] {error_msg}")
            return SearchResponse(
                query=query,
                results=[],
                provider=self.name,
                success=False,
                error_message=error_msg
            )
        except requests.exceptions.RequestException as e:
            error_msg = f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"
            logger.error(f"[Brave] {error_msg}")
            return SearchResponse(
                query=query,
                results=[],
                provider=self.name,
                success=False,
                error_message=error_msg
            )
        except Exception as e:
            error_msg = f"æœªçŸ¥é”™è¯¯: {str(e)}"
            logger.error(f"[Brave] {error_msg}")
            return SearchResponse(
                query=query,
                results=[],
                provider=self.name,
                success=False,
                error_message=error_msg
            )

    def _parse_error(self, response) -> str:
        """è§£æé”™è¯¯å“åº”"""
        try:
            if response.headers.get('content-type', '').startswith('application/json'):
                error_data = response.json()
                # Brave API è¿”å›çš„é”™è¯¯æ ¼å¼
                if 'message' in error_data:
                    return error_data['message']
                if 'error' in error_data:
                    return error_data['error']
                return str(error_data)
            return response.text[:200]
        except:
            return f"HTTP {response.status_code}: {response.text[:200]}"

    @staticmethod
    def _extract_domain(url: str) -> str:
        """ä» URL æå–åŸŸåä½œä¸ºæ¥æº"""
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            domain = parsed.netloc.replace('www.', '')
            return domain or 'æœªçŸ¥æ¥æº'
        except:
            return 'æœªçŸ¥æ¥æº'


class SearchService:
    """
    æœç´¢æœåŠ¡
    
    åŠŸèƒ½ï¼š
    1. ç®¡ç†å¤šä¸ªæœç´¢å¼•æ“
    2. è‡ªåŠ¨æ•…éšœè½¬ç§»
    3. ç»“æœèšåˆå’Œæ ¼å¼åŒ–
    4. æ•°æ®æºå¤±è´¥æ—¶çš„å¢å¼ºæœç´¢ï¼ˆè‚¡ä»·ã€èµ°åŠ¿ç­‰ï¼‰
    5. æ¸¯è‚¡/ç¾è‚¡è‡ªåŠ¨ä½¿ç”¨è‹±æ–‡æœç´¢å…³é”®è¯
    """
    
    # å¢å¼ºæœç´¢å…³é”®è¯æ¨¡æ¿ï¼ˆAè‚¡ ä¸­æ–‡ï¼‰
    ENHANCED_SEARCH_KEYWORDS = [
        "{name} è‚¡ç¥¨ ä»Šæ—¥ è‚¡ä»·",
        "{name} {code} æœ€æ–° è¡Œæƒ… èµ°åŠ¿",
        "{name} è‚¡ç¥¨ åˆ†æ èµ°åŠ¿å›¾",
        "{name} Kçº¿ æŠ€æœ¯åˆ†æ",
        "{name} {code} æ¶¨è·Œ æˆäº¤é‡",
    ]

    # å¢å¼ºæœç´¢å…³é”®è¯æ¨¡æ¿ï¼ˆæ¸¯è‚¡/ç¾è‚¡ è‹±æ–‡ï¼‰
    ENHANCED_SEARCH_KEYWORDS_EN = [
        "{name} stock price today",
        "{name} {code} latest quote trend",
        "{name} stock analysis chart",
        "{name} technical analysis",
        "{name} {code} performance volume",
    ]
    
    def __init__(
        self,
        bocha_keys: Optional[List[str]] = None,
        tavily_keys: Optional[List[str]] = None,
        brave_keys: Optional[List[str]] = None,
        serpapi_keys: Optional[List[str]] = None,
    ):
        """
        åˆå§‹åŒ–æœç´¢æœåŠ¡

        Args:
            bocha_keys: åšæŸ¥æœç´¢ API Key åˆ—è¡¨
            tavily_keys: Tavily API Key åˆ—è¡¨
            brave_keys: Brave Search API Key åˆ—è¡¨
            serpapi_keys: SerpAPI Key åˆ—è¡¨
        """
        self._providers: List[BaseSearchProvider] = []

        # åˆå§‹åŒ–æœç´¢å¼•æ“ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        # 1. Bocha ä¼˜å…ˆï¼ˆä¸­æ–‡æœç´¢ä¼˜åŒ–ï¼ŒAIæ‘˜è¦ï¼‰
        if bocha_keys:
            self._providers.append(BochaSearchProvider(bocha_keys))
            logger.info(f"å·²é…ç½® Bocha æœç´¢ï¼Œå…± {len(bocha_keys)} ä¸ª API Key")

        # 2. Tavilyï¼ˆå…è´¹é¢åº¦æ›´å¤šï¼Œæ¯æœˆ 1000 æ¬¡ï¼‰
        if tavily_keys:
            self._providers.append(TavilySearchProvider(tavily_keys))
            logger.info(f"å·²é…ç½® Tavily æœç´¢ï¼Œå…± {len(tavily_keys)} ä¸ª API Key")

        # 3. Brave Searchï¼ˆéšç§ä¼˜å…ˆï¼Œå…¨çƒè¦†ç›–ï¼‰
        if brave_keys:
            self._providers.append(BraveSearchProvider(brave_keys))
            logger.info(f"å·²é…ç½® Brave æœç´¢ï¼Œå…± {len(brave_keys)} ä¸ª API Key")

        # 4. SerpAPI ä½œä¸ºå¤‡é€‰ï¼ˆæ¯æœˆ 100 æ¬¡ï¼‰
        if serpapi_keys:
            self._providers.append(SerpAPISearchProvider(serpapi_keys))
            logger.info(f"å·²é…ç½® SerpAPI æœç´¢ï¼Œå…± {len(serpapi_keys)} ä¸ª API Key")
        
        if not self._providers:
            logger.warning("æœªé…ç½®ä»»ä½•æœç´¢å¼•æ“ API Keyï¼Œæ–°é—»æœç´¢åŠŸèƒ½å°†ä¸å¯ç”¨")

        # In-memory search result cache: {cache_key: (timestamp, SearchResponse)}
        self._cache: Dict[str, Tuple[float, 'SearchResponse']] = {}
        # Default cache TTL in seconds (10 minutes)
        self._cache_ttl: int = 600
    
    @staticmethod
    def _is_foreign_stock(stock_code: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ¸¯è‚¡æˆ–ç¾è‚¡"""
        import re
        code = stock_code.strip()
        # ç¾è‚¡ï¼š1-5ä¸ªå¤§å†™å­—æ¯ï¼Œå¯èƒ½åŒ…å«ç‚¹ï¼ˆå¦‚ BRK.Bï¼‰
        if re.match(r'^[A-Za-z]{1,5}(\.[A-Za-z])?$', code):
            return True
        # æ¸¯è‚¡ï¼šå¸¦ hk å‰ç¼€æˆ– 5ä½çº¯æ•°å­—
        lower = code.lower()
        if lower.startswith('hk'):
            return True
        if code.isdigit() and len(code) == 5:
            return True
        return False

    @property
    def is_available(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„æœç´¢å¼•æ“"""
        return any(p.is_available for p in self._providers)

    def _cache_key(self, query: str, max_results: int, days: int) -> str:
        """Build a cache key from query parameters."""
        return f"{query}|{max_results}|{days}"

    def _get_cached(self, key: str) -> Optional['SearchResponse']:
        """Return cached SearchResponse if still valid, else None."""
        entry = self._cache.get(key)
        if entry is None:
            return None
        ts, response = entry
        if time.time() - ts > self._cache_ttl:
            del self._cache[key]
            return None
        logger.debug(f"Search cache hit: {key[:60]}...")
        return response

    def _put_cache(self, key: str, response: 'SearchResponse') -> None:
        """Store a successful SearchResponse in cache."""
        # Hard cap: evict oldest entries when cache exceeds limit
        _MAX_CACHE_SIZE = 500
        if len(self._cache) >= _MAX_CACHE_SIZE:
            now = time.time()
            # First pass: remove expired entries
            expired = [k for k, (ts, _) in self._cache.items() if now - ts > self._cache_ttl]
            for k in expired:
                del self._cache[k]
            # Second pass: if still over limit, evict oldest entries (FIFO)
            if len(self._cache) >= _MAX_CACHE_SIZE:
                excess = len(self._cache) - _MAX_CACHE_SIZE + 1
                oldest = sorted(self._cache.keys(), key=lambda k: self._cache[k][0])[:excess]
                for k in oldest:
                    del self._cache[k]
        self._cache[key] = (time.time(), response)
    
    def search_stock_news(
        self,
        stock_code: str,
        stock_name: str,
        max_results: int = 5,
        focus_keywords: Optional[List[str]] = None
    ) -> SearchResponse:
        """
        æœç´¢è‚¡ç¥¨ç›¸å…³æ–°é—»
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            stock_name: è‚¡ç¥¨åç§°
            max_results: æœ€å¤§è¿”å›ç»“æœæ•°
            focus_keywords: é‡ç‚¹å…³æ³¨çš„å…³é”®è¯åˆ—è¡¨
            
        Returns:
            SearchResponse å¯¹è±¡
        """
        # æ™ºèƒ½ç¡®å®šæœç´¢æ—¶é—´èŒƒå›´
        # ç­–ç•¥ï¼š
        # 1. å‘¨äºŒè‡³å‘¨äº”ï¼šæœç´¢è¿‘1å¤©ï¼ˆ24å°æ—¶ï¼‰
        # 2. å‘¨å…­ã€å‘¨æ—¥ï¼šæœç´¢è¿‘2-3å¤©ï¼ˆè¦†ç›–å‘¨æœ«ï¼‰
        # 3. å‘¨ä¸€ï¼šæœç´¢è¿‘3å¤©ï¼ˆè¦†ç›–å‘¨æœ«ï¼‰
        today_weekday = datetime.now().weekday()
        if today_weekday == 0: # å‘¨ä¸€
            search_days = 3
        elif today_weekday >= 5: # å‘¨å…­(5)ã€å‘¨æ—¥(6)
            search_days = 2
        else: # å‘¨äºŒ(1) - å‘¨äº”(4)
            search_days = 1

        # æ„å»ºæœç´¢æŸ¥è¯¢ï¼ˆä¼˜åŒ–æœç´¢æ•ˆæœï¼‰
        is_foreign = self._is_foreign_stock(stock_code)
        if focus_keywords:
            # å¦‚æœæä¾›äº†å…³é”®è¯ï¼Œç›´æ¥ä½¿ç”¨å…³é”®è¯ä½œä¸ºæŸ¥è¯¢
            query = " ".join(focus_keywords)
        elif is_foreign:
            # æ¸¯è‚¡/ç¾è‚¡ä½¿ç”¨è‹±æ–‡æœç´¢å…³é”®è¯
            query = f"{stock_name} {stock_code} stock latest news"
        else:
            # é»˜è®¤ä¸»æŸ¥è¯¢ï¼šè‚¡ç¥¨åç§° + æ ¸å¿ƒå…³é”®è¯
            query = f"{stock_name} {stock_code} è‚¡ç¥¨ æœ€æ–°æ¶ˆæ¯"

        logger.info(f"æœç´¢è‚¡ç¥¨æ–°é—»: {stock_name}({stock_code}), query='{query}', æ—¶é—´èŒƒå›´: è¿‘{search_days}å¤©")

        # Check cache first
        cache_key = self._cache_key(query, max_results, search_days)
        cached = self._get_cached(cache_key)
        if cached is not None:
            logger.info(f"ä½¿ç”¨ç¼“å­˜æœç´¢ç»“æœ: {stock_name}({stock_code})")
            return cached

        # ä¾æ¬¡å°è¯•å„ä¸ªæœç´¢å¼•æ“
        for provider in self._providers:
            if not provider.is_available:
                continue
            
            response = provider.search(query, max_results, days=search_days)
            
            if response.success and response.results:
                logger.info(f"ä½¿ç”¨ {provider.name} æœç´¢æˆåŠŸ")
                self._put_cache(cache_key, response)
                return response
            else:
                logger.warning(f"{provider.name} æœç´¢å¤±è´¥: {response.error_message}ï¼Œå°è¯•ä¸‹ä¸€ä¸ªå¼•æ“")
        
        # æ‰€æœ‰å¼•æ“éƒ½å¤±è´¥
        return SearchResponse(
            query=query,
            results=[],
            provider="None",
            success=False,
            error_message="æ‰€æœ‰æœç´¢å¼•æ“éƒ½ä¸å¯ç”¨æˆ–æœç´¢å¤±è´¥"
        )
    
    def search_stock_events(
        self,
        stock_code: str,
        stock_name: str,
        event_types: Optional[List[str]] = None
    ) -> SearchResponse:
        """
        æœç´¢è‚¡ç¥¨ç‰¹å®šäº‹ä»¶ï¼ˆå¹´æŠ¥é¢„å‘Šã€å‡æŒç­‰ï¼‰
        
        ä¸“é—¨é’ˆå¯¹äº¤æ˜“å†³ç­–ç›¸å…³çš„é‡è¦äº‹ä»¶è¿›è¡Œæœç´¢
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            stock_name: è‚¡ç¥¨åç§°
            event_types: äº‹ä»¶ç±»å‹åˆ—è¡¨
            
        Returns:
            SearchResponse å¯¹è±¡
        """
        if event_types is None:
            if self._is_foreign_stock(stock_code):
                event_types = ["earnings report", "insider selling", "quarterly results"]
            else:
                event_types = ["å¹´æŠ¥é¢„å‘Š", "å‡æŒå…¬å‘Š", "ä¸šç»©å¿«æŠ¥"]
        
        # æ„å»ºé’ˆå¯¹æ€§æŸ¥è¯¢
        event_query = " OR ".join(event_types)
        query = f"{stock_name} ({event_query})"
        
        logger.info(f"æœç´¢è‚¡ç¥¨äº‹ä»¶: {stock_name}({stock_code}) - {event_types}")
        
        # ä¾æ¬¡å°è¯•å„ä¸ªæœç´¢å¼•æ“
        for provider in self._providers:
            if not provider.is_available:
                continue
            
            response = provider.search(query, max_results=5)
            
            if response.success:
                return response
        
        return SearchResponse(
            query=query,
            results=[],
            provider="None",
            success=False,
            error_message="äº‹ä»¶æœç´¢å¤±è´¥"
        )
    
    def search_comprehensive_intel(
        self,
        stock_code: str,
        stock_name: str,
        max_searches: int = 3
    ) -> Dict[str, SearchResponse]:
        """
        å¤šç»´åº¦æƒ…æŠ¥æœç´¢ï¼ˆåŒæ—¶ä½¿ç”¨å¤šä¸ªå¼•æ“ã€å¤šä¸ªç»´åº¦ï¼‰
        
        æœç´¢ç»´åº¦ï¼š
        1. æœ€æ–°æ¶ˆæ¯ - è¿‘æœŸæ–°é—»åŠ¨æ€
        2. é£é™©æ’æŸ¥ - å‡æŒã€å¤„ç½šã€åˆ©ç©º
        3. ä¸šç»©é¢„æœŸ - å¹´æŠ¥é¢„å‘Šã€ä¸šç»©å¿«æŠ¥
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            stock_name: è‚¡ç¥¨åç§°
            max_searches: æœ€å¤§æœç´¢æ¬¡æ•°
            
        Returns:
            {ç»´åº¦åç§°: SearchResponse} å­—å…¸
        """
        results = {}
        search_count = 0
        
        # æ ¹æ®è‚¡ç¥¨ç±»å‹é€‰æ‹©æœç´¢å…³é”®è¯è¯­è¨€
        is_foreign = self._is_foreign_stock(stock_code)

        # å®šä¹‰æœç´¢ç»´åº¦
        if is_foreign:
            search_dimensions = [
                {
                    'name': 'latest_news',
                    'query': f"{stock_name} {stock_code} latest news events",
                    'desc': 'æœ€æ–°æ¶ˆæ¯'
                },
                {
                    'name': 'market_analysis',
                    'query': f"{stock_name} analyst rating target price report",
                    'desc': 'æœºæ„åˆ†æ'
                },
                {
                    'name': 'risk_check',
                    'query': f"{stock_name} risk insider selling lawsuit litigation",
                    'desc': 'é£é™©æ’æŸ¥'
                },
                {
                    'name': 'earnings',
                    'query': f"{stock_name} earnings revenue profit growth forecast",
                    'desc': 'ä¸šç»©é¢„æœŸ'
                },
                {
                    'name': 'industry',
                    'query': f"{stock_name} industry competitors market share outlook",
                    'desc': 'è¡Œä¸šåˆ†æ'
                },
            ]
        else:
            search_dimensions = [
                {
                    'name': 'latest_news',
                    'query': f"{stock_name} {stock_code} æœ€æ–° æ–°é—» é‡å¤§ äº‹ä»¶",
                    'desc': 'æœ€æ–°æ¶ˆæ¯'
                },
                {
                    'name': 'market_analysis',
                    'query': f"{stock_name} ç ”æŠ¥ ç›®æ ‡ä»· è¯„çº§ æ·±åº¦åˆ†æ",
                    'desc': 'æœºæ„åˆ†æ'
                },
                {
                    'name': 'risk_check',
                    'query': f"{stock_name} å‡æŒ å¤„ç½š è¿è§„ è¯‰è®¼ åˆ©ç©º é£é™©",
                    'desc': 'é£é™©æ’æŸ¥'
                },
                {
                    'name': 'earnings',
                    'query': f"{stock_name} ä¸šç»©é¢„å‘Š è´¢æŠ¥ è¥æ”¶ å‡€åˆ©æ¶¦ åŒæ¯”å¢é•¿",
                    'desc': 'ä¸šç»©é¢„æœŸ'
                },
                {
                    'name': 'industry',
                    'query': f"{stock_name} æ‰€åœ¨è¡Œä¸š ç«äº‰å¯¹æ‰‹ å¸‚åœºä»½é¢ è¡Œä¸šå‰æ™¯",
                    'desc': 'è¡Œä¸šåˆ†æ'
                },
            ]
        
        logger.info(f"å¼€å§‹å¤šç»´åº¦æƒ…æŠ¥æœç´¢: {stock_name}({stock_code})")
        
        # è½®æµä½¿ç”¨ä¸åŒçš„æœç´¢å¼•æ“
        provider_index = 0
        
        for dim in search_dimensions:
            if search_count >= max_searches:
                break
            
            # é€‰æ‹©æœç´¢å¼•æ“ï¼ˆè½®æµä½¿ç”¨ï¼‰
            available_providers = [p for p in self._providers if p.is_available]
            if not available_providers:
                break
            
            provider = available_providers[provider_index % len(available_providers)]
            provider_index += 1
            
            logger.info(f"[æƒ…æŠ¥æœç´¢] {dim['desc']}: ä½¿ç”¨ {provider.name}")
            
            response = provider.search(dim['query'], max_results=3)
            results[dim['name']] = response
            search_count += 1
            
            if response.success:
                logger.info(f"[æƒ…æŠ¥æœç´¢] {dim['desc']}: è·å– {len(response.results)} æ¡ç»“æœ")
            else:
                logger.warning(f"[æƒ…æŠ¥æœç´¢] {dim['desc']}: æœç´¢å¤±è´¥ - {response.error_message}")
            
            # çŸ­æš‚å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(0.5)
        
        return results
    
    def format_intel_report(self, intel_results: Dict[str, SearchResponse], stock_name: str) -> str:
        """
        æ ¼å¼åŒ–æƒ…æŠ¥æœç´¢ç»“æœä¸ºæŠ¥å‘Š
        
        Args:
            intel_results: å¤šç»´åº¦æœç´¢ç»“æœ
            stock_name: è‚¡ç¥¨åç§°
            
        Returns:
            æ ¼å¼åŒ–çš„æƒ…æŠ¥æŠ¥å‘Šæ–‡æœ¬
        """
        lines = [f"ã€{stock_name} æƒ…æŠ¥æœç´¢ç»“æœã€‘"]
        
        # ç»´åº¦å±•ç¤ºé¡ºåº
        display_order = ['latest_news', 'market_analysis', 'risk_check', 'earnings', 'industry']
        
        for dim_name in display_order:
            if dim_name not in intel_results:
                continue
                
            resp = intel_results[dim_name]
            
            # è·å–ç»´åº¦æè¿°
            dim_desc = dim_name
            if dim_name == 'latest_news': dim_desc = 'ğŸ“° æœ€æ–°æ¶ˆæ¯'
            elif dim_name == 'market_analysis': dim_desc = 'ğŸ“ˆ æœºæ„åˆ†æ'
            elif dim_name == 'risk_check': dim_desc = 'âš ï¸ é£é™©æ’æŸ¥'
            elif dim_name == 'earnings': dim_desc = 'ğŸ“Š ä¸šç»©é¢„æœŸ'
            elif dim_name == 'industry': dim_desc = 'ğŸ­ è¡Œä¸šåˆ†æ'
            
            lines.append(f"\n{dim_desc} (æ¥æº: {resp.provider}):")
            if resp.success and resp.results:
                # å¢åŠ æ˜¾ç¤ºæ¡æ•°
                for i, r in enumerate(resp.results[:4], 1):
                    date_str = f" [{r.published_date}]" if r.published_date else ""
                    lines.append(f"  {i}. {r.title}{date_str}")
                    # å¦‚æœæ‘˜è¦å¤ªçŸ­ï¼Œå¯èƒ½ä¿¡æ¯é‡ä¸è¶³
                    snippet = r.snippet[:150] if len(r.snippet) > 20 else r.snippet
                    lines.append(f"     {snippet}...")
            else:
                lines.append("  æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯")
        
        return "\n".join(lines)
    
    def batch_search(
        self,
        stocks: List[Dict[str, str]],
        max_results_per_stock: int = 3,
        delay_between: float = 1.0
    ) -> Dict[str, SearchResponse]:
        """
        Batch search news for multiple stocks.
        
        Args:
            stocks: List of stocks
            max_results_per_stock: Max results per stock
            delay_between: Delay between searches (seconds)
            
        Returns:
            Dict of results
        """
        results = {}
        
        for i, stock in enumerate(stocks):
            if i > 0:
                time.sleep(delay_between)
            
            code = stock.get('code', '')
            name = stock.get('name', '')
            
            response = self.search_stock_news(code, name, max_results_per_stock)
            results[code] = response
        
        return results

    def search_stock_price_fallback(
        self,
        stock_code: str,
        stock_name: str,
        max_attempts: int = 3,
        max_results: int = 5
    ) -> SearchResponse:
        """
        Enhance search when data sources fail.
        
        When all data sources (efinance, akshare, tushare, baostock, etc.) fail to get
        stock data, use search engines to find stock trends and price info as supplemental data for AI analysis.
        
        Strategy:
        1. Search using multiple keyword templates
        2. Try all available search engines for each keyword
        3. Aggregate and deduplicate results
        
        Args:
            stock_code: Stock Code
            stock_name: Stock Name
            max_attempts: Max search attempts (using different keywords)
            max_results: Max results to return
            
        Returns:
            SearchResponse object with aggregated results
        """

        if not self.is_available:
            return SearchResponse(
                query=f"{stock_name} è‚¡ä»·èµ°åŠ¿",
                results=[],
                provider="None",
                success=False,
                error_message="æœªé…ç½®æœç´¢å¼•æ“ API Key"
            )
        
        logger.info(f"[å¢å¼ºæœç´¢] æ•°æ®æºå¤±è´¥ï¼Œå¯åŠ¨å¢å¼ºæœç´¢: {stock_name}({stock_code})")
        
        all_results = []
        seen_urls = set()
        successful_providers = []
        
        # ä½¿ç”¨å¤šä¸ªå…³é”®è¯æ¨¡æ¿æœç´¢
        is_foreign = self._is_foreign_stock(stock_code)
        keywords = self.ENHANCED_SEARCH_KEYWORDS_EN if is_foreign else self.ENHANCED_SEARCH_KEYWORDS
        for i, keyword_template in enumerate(keywords[:max_attempts]):
            query = keyword_template.format(name=stock_name, code=stock_code)
            
            logger.info(f"[å¢å¼ºæœç´¢] ç¬¬ {i+1}/{max_attempts} æ¬¡æœç´¢: {query}")
            
            # ä¾æ¬¡å°è¯•å„ä¸ªæœç´¢å¼•æ“
            for provider in self._providers:
                if not provider.is_available:
                    continue
                
                try:
                    response = provider.search(query, max_results=3)
                    
                    if response.success and response.results:
                        # å»é‡å¹¶æ·»åŠ ç»“æœ
                        for result in response.results:
                            if result.url not in seen_urls:
                                seen_urls.add(result.url)
                                all_results.append(result)
                                
                        if provider.name not in successful_providers:
                            successful_providers.append(provider.name)
                        
                        logger.info(f"[å¢å¼ºæœç´¢] {provider.name} è¿”å› {len(response.results)} æ¡ç»“æœ")
                        break  # æˆåŠŸåè·³åˆ°ä¸‹ä¸€ä¸ªå…³é”®è¯
                    else:
                        logger.debug(f"[å¢å¼ºæœç´¢] {provider.name} æ— ç»“æœæˆ–å¤±è´¥")
                        
                except Exception as e:
                    logger.warning(f"[å¢å¼ºæœç´¢] {provider.name} æœç´¢å¼‚å¸¸: {e}")
                    continue
            
            # çŸ­æš‚å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            if i < max_attempts - 1:
                time.sleep(0.5)
        
        # æ±‡æ€»ç»“æœ
        if all_results:
            # æˆªå–å‰ max_results æ¡
            final_results = all_results[:max_results]
            provider_str = ", ".join(successful_providers) if successful_providers else "None"
            
            logger.info(f"[å¢å¼ºæœç´¢] å®Œæˆï¼Œå…±è·å– {len(final_results)} æ¡ç»“æœï¼ˆæ¥æº: {provider_str}ï¼‰")
            
            return SearchResponse(
                query=f"{stock_name}({stock_code}) è‚¡ä»·èµ°åŠ¿",
                results=final_results,
                provider=provider_str,
                success=True,
            )
        else:
            logger.warning(f"[å¢å¼ºæœç´¢] æ‰€æœ‰æœç´¢å‡æœªè¿”å›ç»“æœ")
            return SearchResponse(
                query=f"{stock_name}({stock_code}) è‚¡ä»·èµ°åŠ¿",
                results=[],
                provider="None",
                success=False,
                error_message="å¢å¼ºæœç´¢æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
            )

    def search_stock_with_enhanced_fallback(
        self,
        stock_code: str,
        stock_name: str,
        include_news: bool = True,
        include_price: bool = False,
        max_results: int = 5
    ) -> Dict[str, SearchResponse]:
        """
        ç»¼åˆæœç´¢æ¥å£ï¼ˆæ”¯æŒæ–°é—»å’Œè‚¡ä»·ä¿¡æ¯ï¼‰
        
        å½“ include_price=True æ—¶ï¼Œä¼šåŒæ—¶æœç´¢æ–°é—»å’Œè‚¡ä»·ä¿¡æ¯ã€‚
        ä¸»è¦ç”¨äºæ•°æ®æºå®Œå…¨å¤±è´¥æ—¶çš„å…œåº•æ–¹æ¡ˆã€‚
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            stock_name: è‚¡ç¥¨åç§°
            include_news: æ˜¯å¦æœç´¢æ–°é—»
            include_price: æ˜¯å¦æœç´¢è‚¡ä»·/èµ°åŠ¿ä¿¡æ¯
            max_results: æ¯ç±»æœç´¢çš„æœ€å¤§ç»“æœæ•°
            
        Returns:
            {'news': SearchResponse, 'price': SearchResponse} å­—å…¸
        """
        results = {}
        
        if include_news:
            results['news'] = self.search_stock_news(
                stock_code, 
                stock_name, 
                max_results=max_results
            )
        
        if include_price:
            results['price'] = self.search_stock_price_fallback(
                stock_code,
                stock_name,
                max_attempts=3,
                max_results=max_results
            )
        
        return results

    def format_price_search_context(self, response: SearchResponse) -> str:
        """
        å°†è‚¡ä»·æœç´¢ç»“æœæ ¼å¼åŒ–ä¸º AI åˆ†æä¸Šä¸‹æ–‡
        
        Args:
            response: æœç´¢å“åº”å¯¹è±¡
            
        Returns:
            æ ¼å¼åŒ–çš„æ–‡æœ¬ï¼Œå¯ç›´æ¥ç”¨äº AI åˆ†æ
        """
        if not response.success or not response.results:
            return "ã€è‚¡ä»·èµ°åŠ¿æœç´¢ã€‘æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œè¯·ä»¥å…¶ä»–æ¸ é“æ•°æ®ä¸ºå‡†ã€‚"
        
        lines = [
            f"ã€è‚¡ä»·èµ°åŠ¿æœç´¢ç»“æœã€‘ï¼ˆæ¥æº: {response.provider}ï¼‰",
            "âš ï¸ æ³¨æ„ï¼šä»¥ä¸‹ä¿¡æ¯æ¥è‡ªç½‘ç»œæœç´¢ï¼Œä»…ä¾›å‚è€ƒï¼Œå¯èƒ½å­˜åœ¨å»¶è¿Ÿæˆ–ä¸å‡†ç¡®ã€‚",
            ""
        ]
        
        for i, result in enumerate(response.results, 1):
            date_str = f" [{result.published_date}]" if result.published_date else ""
            lines.append(f"{i}. ã€{result.source}ã€‘{result.title}{date_str}")
            lines.append(f"   {result.snippet[:200]}...")
            lines.append("")
        
        return "\n".join(lines)


# === ä¾¿æ·å‡½æ•° ===
_search_service: Optional[SearchService] = None


def get_search_service() -> SearchService:
    """è·å–æœç´¢æœåŠ¡å•ä¾‹"""
    global _search_service
    
    if _search_service is None:
        from src.config import get_config
        config = get_config()
        
        _search_service = SearchService(
            bocha_keys=config.bocha_api_keys,
            tavily_keys=config.tavily_api_keys,
            brave_keys=config.brave_api_keys,
            serpapi_keys=config.serpapi_keys,
        )
    
    return _search_service


def reset_search_service() -> None:
    """é‡ç½®æœç´¢æœåŠ¡ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    global _search_service
    _search_service = None


if __name__ == "__main__":
    # æµ‹è¯•æœç´¢æœåŠ¡
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s | %(levelname)-8s | %(name)-20s | %(message)s'
    )
    
    # æ‰‹åŠ¨æµ‹è¯•ï¼ˆéœ€è¦é…ç½® API Keyï¼‰
    service = get_search_service()
    
    if service.is_available:
        print("=== æµ‹è¯•è‚¡ç¥¨æ–°é—»æœç´¢ ===")
        response = service.search_stock_news("300389", "è‰¾æ¯”æ£®")
        print(f"æœç´¢çŠ¶æ€: {'æˆåŠŸ' if response.success else 'å¤±è´¥'}")
        print(f"æœç´¢å¼•æ“: {response.provider}")
        print(f"ç»“æœæ•°é‡: {len(response.results)}")
        print(f"è€—æ—¶: {response.search_time:.2f}s")
        print("\n" + response.to_context())
    else:
        print("æœªé…ç½®æœç´¢å¼•æ“ API Keyï¼Œè·³è¿‡æµ‹è¯•")
