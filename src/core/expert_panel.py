# -*- coding: utf-8 -*-
"""
å¤šæ¨¡å‹ä¸“å®¶ä¼šè¯Šç®¡ç†å™¨ (Expert Panel Manager)

æ”¯æŒåŒæ—¶è°ƒç”¨å¤šä¸ª AI æ¨¡å‹å¯¹åŒä¸€åªè‚¡ç¥¨è¿›è¡Œç‹¬ç«‹åˆ†æï¼Œ
å¹¶æ±‡æ€»å„æ¨¡å‹è§‚ç‚¹ç”Ÿæˆ"ä¸“å®¶ä¼šè¯Š"æŠ¥å‘Šã€‚

ç‰¹æ€§ï¼š
- æ”¯æŒæœ€å¤š 10 ä¸ªé€»è¾‘æ¨¡å‹å¹¶è¡Œåˆ†æ
- é€»è¾‘æ¨¡å‹å†…æ”¯æŒ endpoint æ± è½®è½¬ä¸æ•…éšœåˆ‡æ¢
- è‡ªåŠ¨ç”Ÿæˆå¯¹æ¯”æ±‡æ€»å’Œç»¼åˆç»“è®º
- ä»…åœ¨ç”¨æˆ·æ‰‹åŠ¨è§¦å‘æ—¶æ‰§è¡Œï¼ˆå®šæ—¶ä»»åŠ¡ä¸æ”¯æŒï¼‰
"""

from __future__ import annotations

import json
import logging
import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional
from urllib.parse import urlparse

from src.config import get_config

logger = logging.getLogger(__name__)

# æœ€å¤§å¹¶è¡Œé€»è¾‘æ¨¡å‹æ•°é‡
MAX_MODELS = 10


@dataclass
class ModelEndpoint:
    """é€»è¾‘æ¨¡å‹ä¸‹çš„å•ä¸ª endpoint é…ç½®ã€‚"""

    id: str
    api_key: str
    base_url: Optional[str] = None
    priority: int = 0
    enabled: bool = True
    temperature: Optional[float] = None
    verify_ssl: bool = True
    source_name: Optional[str] = None  # åŸå§‹é…ç½®åç§°ï¼ˆå¦‚ "OpenAIä»£ç†A"ï¼‰


@dataclass
class ModelConfig:
    """é€»è¾‘æ¨¡å‹é…ç½®ï¼ˆåŒæ¨¡å‹å¯å«å¤šä¸ª endpointï¼‰ã€‚"""

    name: str
    provider: str
    model_name: Optional[str] = None
    endpoints: List[ModelEndpoint] = field(default_factory=list)


@dataclass
class ModelResult:
    """å•ä¸ªé€»è¾‘æ¨¡å‹çš„åˆ†æç»“æœã€‚"""

    model_name: str
    success: bool
    score: Optional[int] = None
    advice: Optional[str] = None
    trend: Optional[str] = None
    summary: Optional[str] = None
    confidence: Optional[str] = None
    raw_result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    elapsed_seconds: float = 0.0
    endpoint_tried: List[str] = field(default_factory=list)
    endpoint_used: Optional[str] = None
    fallback_count: int = 0

    def to_dict(self) -> Dict[str, Any]:
        return {
            "model_name": self.model_name,
            "success": self.success,
            "score": self.score,
            "advice": self.advice,
            "trend": self.trend,
            "summary": self.summary,
            "confidence": self.confidence,
            "elapsed_seconds": round(self.elapsed_seconds, 2),
            "raw_result": self.raw_result,
            "error": self.error,
            "endpoint_tried": self.endpoint_tried,
            "endpoint_used": self.endpoint_used,
            "fallback_count": self.fallback_count,
        }


@dataclass
class ExpertPanelResult:
    """ä¸“å®¶ä¼šè¯Šæ±‡æ€»ç»“æœã€‚"""

    stock_code: str
    stock_name: str
    models_used: List[str]
    model_results: List[ModelResult]
    consensus_score: Optional[int] = None
    consensus_advice: Optional[str] = None
    consensus_summary: Optional[str] = None
    consensus_strategy: Optional[Dict[str, Any]] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "stock_code": self.stock_code,
            "stock_name": self.stock_name,
            "models_used": self.models_used,
            "consensus_score": self.consensus_score,
            "consensus_advice": self.consensus_advice,
            "consensus_summary": self.consensus_summary,
            "consensus_strategy": self.consensus_strategy,
            "model_results": [r.to_dict() for r in self.model_results],
        }

    def to_json(self) -> str:
        return json.dumps(self.to_dict(), ensure_ascii=False)


def _safe_float(value: Any, default: Optional[float] = None) -> Optional[float]:
    if value is None or value == "":
        return default
    try:
        return float(value)
    except (TypeError, ValueError):
        return default


def _safe_int(value: Any, default: int = 0) -> int:
    try:
        return int(value)
    except (TypeError, ValueError):
        return default


def _normalize_bool(value: Any, default: bool = True) -> bool:
    if isinstance(value, bool):
        return value
    if isinstance(value, str):
        normalized = value.strip().lower()
        if normalized in {"true", "1", "yes", "on"}:
            return True
        if normalized in {"false", "0", "no", "off"}:
            return False
    return default


def _extract_host_label(base_url: Optional[str]) -> Optional[str]:
    if not base_url:
        return None

    raw = base_url.strip()
    if not raw:
        return None

    if not raw.startswith(("http://", "https://")):
        raw = f"https://{raw}"

    try:
        host = (urlparse(raw).hostname or "").lower()
    except Exception:
        return None

    if not host:
        return None

    prefixes = ["api.", "openai.", "gateway.", "chat."]
    for prefix in prefixes:
        if host.startswith(prefix):
            host = host[len(prefix):]
            break

    if host.startswith("www."):
        host = host[4:]

    return host or None


def _provider_default_name(provider: str) -> str:
    provider_lower = (provider or "openai").strip().lower()
    if provider_lower == "gemini":
        return "Gemini"
    if provider_lower == "openai":
        return "OpenAI-Compatible"
    return provider or "Extra-Model"


def _build_auto_name(
    provider: str,
    model_name: Optional[str],
    endpoints: List[ModelEndpoint],
) -> str:
    if model_name:
        return model_name

    for endpoint in endpoints:
        host = _extract_host_label(endpoint.base_url)
        if host:
            return host

    return _provider_default_name(provider)


def _parse_endpoint(
    endpoint_data: Dict[str, Any],
    fallback_id: str,
    inherited_base_url: Optional[str] = None,
    inherited_temperature: Optional[float] = None,
    inherited_verify_ssl: Optional[bool] = None,
    source_name: Optional[str] = None,
) -> Optional[ModelEndpoint]:
    api_key = (endpoint_data.get("api_key") or "").strip()
    if not api_key:
        return None

    endpoint_id = (endpoint_data.get("id") or fallback_id).strip() or fallback_id
    return ModelEndpoint(
        id=endpoint_id,
        api_key=api_key,
        base_url=(endpoint_data.get("base_url") or inherited_base_url),
        priority=_safe_int(endpoint_data.get("priority"), 0),
        enabled=_normalize_bool(endpoint_data.get("enabled"), True),
        temperature=_safe_float(endpoint_data.get("temperature"), inherited_temperature),
        verify_ssl=_normalize_bool(endpoint_data.get("verify_ssl"), inherited_verify_ssl if inherited_verify_ssl is not None else True),
        source_name=source_name,
    )


def _parse_extra_model_entry(item: Dict[str, Any], index: int) -> Optional[ModelConfig]:
    provider = (item.get("provider") or "openai").strip().lower() or "openai"
    model_name = (item.get("model") or item.get("model_name") or "").strip() or None

    # åŸå§‹é…ç½®åç§°ï¼ˆç”¨äºæ˜¾ç¤ºåœ¨UIä¸­æ ‡è¯†channelæ¥æºï¼‰
    source_name = (item.get("name") or "").strip() or None

    endpoints: List[ModelEndpoint] = []
    inherited_base_url = item.get("base_url")
    inherited_temperature = _safe_float(item.get("temperature"), None)
    inherited_verify_ssl = _normalize_bool(item.get("verify_ssl"), True)

    raw_endpoints = item.get("endpoints")
    if isinstance(raw_endpoints, list):
        for ep_idx, endpoint_item in enumerate(raw_endpoints):
            if not isinstance(endpoint_item, dict):
                continue
            endpoint = _parse_endpoint(
                endpoint_item,
                fallback_id=f"ep-{index + 1}-{ep_idx + 1}",
                inherited_base_url=inherited_base_url,
                inherited_temperature=inherited_temperature,
                inherited_verify_ssl=inherited_verify_ssl,
                source_name=source_name,
            )
            if endpoint:
                endpoints.append(endpoint)
    else:
        endpoint = _parse_endpoint(
            item,
            fallback_id=f"ep-{index + 1}-1",
            inherited_base_url=inherited_base_url,
            inherited_temperature=inherited_temperature,
            inherited_verify_ssl=inherited_verify_ssl,
            source_name=source_name,
        )
        if endpoint:
            endpoints.append(endpoint)

    if not endpoints:
        return None

    raw_name = (item.get("name") or "").strip()
    # ä¼˜å…ˆä½¿ç”¨ model_name ä½œä¸ºæ˜¾ç¤ºåç§°ï¼Œå…¶æ¬¡æ‰æ˜¯é…ç½®åç§°
    logical_name = model_name or raw_name or _build_auto_name(provider=provider, model_name=model_name, endpoints=endpoints)

    return ModelConfig(
        name=logical_name,
        provider=provider,
        model_name=model_name,
        endpoints=endpoints,
    )


def _create_single_endpoint_model(
    name: Optional[str],
    provider: str,
    model_name: Optional[str],
    api_key: str,
    base_url: Optional[str] = None,
    temperature: Optional[float] = None,
    verify_ssl: bool = True,
    endpoint_id: str = "primary",
) -> ModelConfig:
    source_name = (name or "").strip()

    # å…ˆåˆ›å»º endpointï¼Œç„¶åå†ç¡®å®š logical_name
    endpoint = ModelEndpoint(
        id=endpoint_id,
        api_key=api_key,
        base_url=base_url,
        priority=0,
        enabled=True,
        temperature=temperature,
        verify_ssl=verify_ssl,
        source_name=source_name or None,
    )

    # ä¼˜å…ˆä½¿ç”¨ model_name ä½œä¸ºæ˜¾ç¤ºåç§°ï¼Œå…¶æ¬¡æ‰æ˜¯ source_name æˆ–è‡ªåŠ¨ç”Ÿæˆ
    logical_name = model_name or source_name or _build_auto_name(provider=provider, model_name=model_name, endpoints=[endpoint])

    return ModelConfig(
        name=logical_name,
        provider=provider,
        model_name=model_name,
        endpoints=[endpoint],
    )


def parse_model_configs() -> List[ModelConfig]:
    """
    è§£ææ‰€æœ‰å¯ç”¨çš„é€»è¾‘æ¨¡å‹é…ç½®ã€‚

    æ”¯æŒé¡ºåºï¼š
    1. Gemini (ä¸»é…ç½®)
    2. OpenAI (ä¸»é…ç½®)
    3. EXTRA_AI_MODELS (JSON æ‰¹é‡é…ç½®ï¼Œå…¼å®¹æ—§æ ¼å¼ä¸æ–° endpoints æ± æ ¼å¼)
    4. MODEL_N_... (å‘½åé…ç½®ï¼Œæ”¯æŒ 1-10)

    æŒ‰ç…§ model_name èšåˆï¼Œç›¸åŒæ¨¡å‹çš„ä¸åŒ endpoint åˆå¹¶ä¸ºä¸€ä¸ªé€»è¾‘æ¨¡å‹ã€‚
    """
    configs: List[ModelConfig] = []
    config = get_config()

    # ç”¨äºæŒ‰ model_name èšåˆçš„ä¸´æ—¶å­—å…¸
    model_groups: Dict[str, List[ModelConfig]] = {}

    # ç”¨äºæŒ‰ model_name èšåˆçš„ä¸´æ—¶å­—å…¸
    model_groups: Dict[str, List[ModelConfig]] = {}

    def add_to_group(cfg: ModelConfig):
        """å°†é…ç½®æ·»åŠ åˆ°å¯¹åº”çš„ model_name åˆ†ç»„"""
        key = cfg.model_name or cfg.name  # å¦‚æœæ²¡æœ‰ model_nameï¼Œç”¨ name ä½œä¸º key
        if key not in model_groups:
            model_groups[key] = []
        model_groups[key].append(cfg)

    # 1. Gemini
    gemini_key = config.gemini_api_key
    if gemini_key and not gemini_key.startswith("your_") and len(gemini_key) > 10:
        add_to_group(_create_single_endpoint_model(
            name="Gemini",
            provider="gemini",
            model_name=config.gemini_model or "gemini-pro",
            api_key=gemini_key,
            temperature=config.gemini_temperature,
            endpoint_id="gemini-primary",
        ))

    # 2. OpenAI (å®˜æ–¹æˆ–å…¼å®¹)
    openai_key = config.openai_api_key
    if openai_key and not openai_key.startswith("your_") and len(openai_key) > 10:
        model_name = config.openai_model or "gpt-4o-mini"
        add_to_group(_create_single_endpoint_model(
            name="OpenAI",
            provider="openai",
            model_name=model_name,
            api_key=openai_key,
            base_url=config.openai_base_url,
            temperature=config.openai_temperature,
            verify_ssl=config.openai_verify_ssl,
            endpoint_id="openai-primary",
        ))

    # 3. EXTRA_AI_MODELS (JSON æ‰¹é‡)
    if config.extra_ai_models:
        try:
            extra_list = json.loads(config.extra_ai_models)
            if isinstance(extra_list, list):
                for index, item in enumerate(extra_list):
                    if not isinstance(item, dict):
                        continue
                    parsed = _parse_extra_model_entry(item, index=index)
                    if parsed:
                        add_to_group(parsed)
        except Exception as e:
            logger.warning(f"è§£æ EXTRA_AI_MODELS å¤±è´¥: {e}")

    # 4. MODEL_N_... (æ”¯æŒ 1-10)
    for i in range(1, 11):
        key = os.getenv(f"MODEL_{i}_API_KEY", "").strip()
        if not key or key.startswith("your_") or len(key) <= 10:
            continue

        provider = os.getenv(f"MODEL_{i}_PROVIDER", "openai").strip().lower() or "openai"
        model_name = os.getenv(f"MODEL_{i}_NAME", "gpt-4o-mini").strip() or "gpt-4o-mini"
        base_url = os.getenv(f"MODEL_{i}_BASE_URL") or None

        add_to_group(_create_single_endpoint_model(
            name=os.getenv(f"MODEL_{i}_DISPLAY_NAME", f"Model-{i}"),
            provider=provider,
            model_name=model_name,
            api_key=key,
            base_url=base_url,
            temperature=_safe_float(os.getenv(f"MODEL_{i}_TEMPERATURE", "0.7"), 0.7),
            endpoint_id=f"model-{i}-primary",
        ))

    # 5. æŒ‰ model_name èšåˆåˆå¹¶
    merged_configs: List[ModelConfig] = []
    for model_name, group in model_groups.items():
        if len(group) == 1:
            # åªæœ‰ä¸€ä¸ªé…ç½®ï¼Œç›´æ¥ä½¿ç”¨
            merged_configs.append(group[0])
        else:
            # å¤šä¸ªé…ç½®ï¼Œåˆå¹¶ endpoints
            all_endpoints: List[ModelEndpoint] = []
            for cfg in group:
                all_endpoints.extend(cfg.endpoints)

            # æŒ‰ä¼˜å…ˆçº§æ’åº
            all_endpoints.sort(key=lambda ep: ep.priority, reverse=True)

            # ä½¿ç”¨ç¬¬ä¸€ä¸ªé…ç½®çš„ä¿¡æ¯ä½œä¸ºåŸºç¡€
            primary = group[0]
            merged = ModelConfig(
                name=model_name,  # ä½¿ç”¨ model_name ä½œä¸ºæ˜¾ç¤ºåç§°
                provider=primary.provider,
                model_name=model_name,
                endpoints=all_endpoints,
            )
            merged_configs.append(merged)

    return merged_configs[:MAX_MODELS]


def _is_endpoint_switchable_error(error_text: str) -> bool:
    lowered = (error_text or "").lower()
    if any(code in lowered for code in ["401", "403", "429"]):
        return True
    if any(marker in lowered for marker in ["500", "502", "503", "504"]):
        return True
    if any(marker in lowered for marker in ["timeout", "timed out", "connect", "connection", "network", "ssl"]):
        return True
    return False


def _get_gemini_analyzer_cls():
    from src.analyzer import GeminiAnalyzer

    return GeminiAnalyzer


def _run_single_model(
    model_config: ModelConfig,
    context: Dict[str, Any],
    news_context: Optional[str],
) -> ModelResult:
    """ä½¿ç”¨å•ä¸ªé€»è¾‘æ¨¡å‹æ‰§è¡Œåˆ†æï¼ˆåœ¨ endpoint æ± å†…è½®è½¬ï¼‰ã€‚"""
    start = time.time()
    endpoint_tried: List[str] = []

    endpoints = sorted(
        [endpoint for endpoint in model_config.endpoints if endpoint.enabled],
        key=lambda item: item.priority,
        reverse=True,
    )

    if not endpoints:
        elapsed = time.time() - start
        return ModelResult(
            model_name=model_config.name,
            success=False,
            error="æ²¡æœ‰å¯ç”¨ endpointï¼ˆå…¨éƒ¨è¢«ç¦ç”¨æˆ–æœªé…ç½®ï¼‰",
            elapsed_seconds=elapsed,
            endpoint_tried=[],
            fallback_count=0,
        )

    last_error: Optional[str] = None

    for index, endpoint in enumerate(endpoints):
        endpoint_tried.append(endpoint.id)
        params = {
            "name": model_config.name,
            "provider": model_config.provider,
            "api_key": endpoint.api_key,
            "base_url": endpoint.base_url,
            "model": model_config.model_name,
            "temperature": endpoint.temperature if endpoint.temperature is not None else 0.7,
            "verify_ssl": endpoint.verify_ssl,
        }

        try:
            analyzer_cls = _get_gemini_analyzer_cls()
            analyzer = analyzer_cls(model_params=params)
            if not analyzer.is_available():
                error_text = "åˆ†æå™¨åˆå§‹åŒ–å¤±è´¥æˆ– API Key æ— æ•ˆ"
                last_error = error_text
                logger.warning("[ä¸“å®¶ä¼šè¯Š] %s endpoint=%s åˆå§‹åŒ–å¤±è´¥", model_config.name, endpoint.id)
                continue

            result = analyzer.analyze(context, news_context)
            if getattr(result, "success", True):
                elapsed = time.time() - start
                return ModelResult(
                    model_name=model_config.name,
                    success=True,
                    score=result.sentiment_score,
                    advice=result.operation_advice,
                    trend=result.trend_prediction,
                    summary=result.analysis_summary,
                    confidence=result.confidence_level if hasattr(result, "confidence_level") else None,
                    raw_result=result.to_dict() if hasattr(result, "to_dict") else None,
                    elapsed_seconds=elapsed,
                    endpoint_tried=endpoint_tried,
                    endpoint_used=endpoint.id,
                    fallback_count=index,
                )

            error_text = getattr(result, "error_message", None) or "æ¨¡å‹è¿”å›å¤±è´¥"
            last_error = error_text
            if _is_endpoint_switchable_error(error_text):
                logger.warning(
                    "[ä¸“å®¶ä¼šè¯Š] %s endpoint=%s å¤±è´¥ï¼Œåˆ‡æ¢ä¸‹ä¸€ endpoint: %s",
                    model_config.name,
                    endpoint.id,
                    error_text,
                )
                continue

            elapsed = time.time() - start
            return ModelResult(
                model_name=model_config.name,
                success=False,
                error=str(error_text)[:200],
                elapsed_seconds=elapsed,
                endpoint_tried=endpoint_tried,
                endpoint_used=endpoint.id,
                fallback_count=index,
            )

        except Exception as e:
            error_text = str(e)[:400]
            last_error = error_text
            logger.warning(
                "[ä¸“å®¶ä¼šè¯Š] %s endpoint=%s å¼‚å¸¸ï¼Œå‡†å¤‡åˆ‡æ¢: %s",
                model_config.name,
                endpoint.id,
                error_text,
            )
            if not _is_endpoint_switchable_error(error_text):
                elapsed = time.time() - start
                return ModelResult(
                    model_name=model_config.name,
                    success=False,
                    error=error_text[:200],
                    elapsed_seconds=elapsed,
                    endpoint_tried=endpoint_tried,
                    endpoint_used=endpoint.id,
                    fallback_count=index,
                )

    elapsed = time.time() - start
    return ModelResult(
        model_name=model_config.name,
        success=False,
        error=(last_error or "æ‰€æœ‰ endpoint å‡å¤±è´¥")[:200],
        elapsed_seconds=elapsed,
        endpoint_tried=endpoint_tried,
        endpoint_used=None,
        fallback_count=max(0, len(endpoint_tried) - 1),
    )


def _compute_consensus(results: List[ModelResult]) -> Dict[str, Any]:
    """è®¡ç®—å¤šæ¨¡å‹å…±è¯†ç»“è®ºã€‚"""
    successful = [r for r in results if r.success and r.score is not None]
    if not successful:
        return {
            "score": None,
            "advice": "æ•°æ®ä¸è¶³",
            "summary": "æ‰€æœ‰æ¨¡å‹åˆ†æå‡å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆå…±è¯†ç»“è®ºã€‚",
            "strategy": None,
        }

    avg_score = round(sum(r.score for r in successful) / len(successful))

    advice_counts: Dict[str, int] = {}
    valid_strategies: List[Dict[str, Any]] = []

    for r in successful:
        if r.advice:
            advice_counts[r.advice] = advice_counts.get(r.advice, 0) + 1

        if r.raw_result and r.raw_result.get("dashboard") and r.raw_result["dashboard"].get("battle_plan"):
            strategies = r.raw_result["dashboard"]["battle_plan"].get("sniper_points")
            if strategies:
                valid_strategies.append(strategies)

    top_advice = max(advice_counts, key=advice_counts.get) if advice_counts else "è§‚æœ›"

    consensus_strategy = None
    if valid_strategies:
        matching_models = [
            r
            for r in successful
            if r.advice == top_advice and r.raw_result and r.raw_result.get("dashboard")
        ]
        if matching_models:
            best_model = max(matching_models, key=lambda x: x.score or 0)
            if best_model.raw_result:
                consensus_strategy = best_model.raw_result["dashboard"]["battle_plan"].get("sniper_points")

        if not consensus_strategy:
            consensus_strategy = valid_strategies[0]

    agree_count = advice_counts.get(top_advice, 0)
    total = len(successful)
    if agree_count == total:
        consensus_text = f"å…¨éƒ¨ {total} ä½ä¸“å®¶ä¸€è‡´å»ºè®®ã€{top_advice}ã€‘"
    else:
        consensus_text = f"{agree_count}/{total} ä½ä¸“å®¶å»ºè®®ã€{top_advice}ã€‘"
        dissenting = [r for r in successful if r.advice != top_advice]
        if dissenting:
            alt_views = ", ".join(f"{r.model_name}å»ºè®®{r.advice}" for r in dissenting)
            consensus_text += f"ï¼Œä½† {alt_views}"

    scores = [r.score for r in successful if r.score is not None]
    score_range = f"è¯„åˆ†åŒºé—´: {min(scores)}-{max(scores)}, å‡å€¼: {avg_score}"

    summary = f"ğŸ“Š {consensus_text}ã€‚{score_range}ã€‚"

    return {
        "score": avg_score,
        "advice": top_advice,
        "summary": summary,
        "strategy": consensus_strategy,
    }


def run_expert_panel(
    context: Dict[str, Any],
    news_context: Optional[str] = None,
    selected_models: Optional[List[str]] = None,
    max_workers: int = 3,
) -> ExpertPanelResult:
    """æ‰§è¡Œä¸“å®¶ä¼šè¯Šåˆ†æã€‚"""
    stock_code = context.get("code", "Unknown")
    stock_name = context.get("stock_name", stock_code)

    all_configs = parse_model_configs()
    if not all_configs:
        return ExpertPanelResult(
            stock_code=stock_code,
            stock_name=stock_name,
            models_used=[],
            model_results=[],
            consensus_summary="æœªé…ç½®ä»»ä½• AI æ¨¡å‹ï¼Œæ— æ³•æ‰§è¡Œä¸“å®¶ä¼šè¯Šã€‚",
        )

    if selected_models:
        selected_lower = [m.lower() for m in selected_models]
        configs = [c for c in all_configs if c.name.lower() in selected_lower]
        if not configs:
            configs = all_configs
    else:
        configs = all_configs

    configs = configs[:MAX_MODELS]
    model_names = [c.name for c in configs]

    logger.info(f"[ä¸“å®¶ä¼šè¯Š] å¼€å§‹åˆ†æ {stock_name}({stock_code}), æ¨¡å‹: {model_names}")

    model_results: List[ModelResult] = []
    with ThreadPoolExecutor(max_workers=min(max_workers, len(configs))) as executor:
        future_to_model = {
            executor.submit(_run_single_model, cfg, context, news_context): cfg.name for cfg in configs
        }
        for future in as_completed(future_to_model):
            model_name = future_to_model[future]
            try:
                result = future.result(timeout=300)
                model_results.append(result)
                status = "âœ…" if result.success else "âŒ"
                logger.info(
                    "[ä¸“å®¶ä¼šè¯Š] %s %s: score=%s, advice=%s, endpoint=%s, fallback_count=%s",
                    status,
                    model_name,
                    result.score,
                    result.advice,
                    result.endpoint_used,
                    result.fallback_count,
                )
            except Exception as e:
                logger.error(f"[ä¸“å®¶ä¼šè¯Š] {model_name} å¼‚å¸¸: {e}")
                model_results.append(
                    ModelResult(
                        model_name=model_name,
                        success=False,
                        error=str(e)[:200],
                    )
                )

    model_results.sort(key=lambda r: model_names.index(r.model_name) if r.model_name in model_names else 999)

    consensus = _compute_consensus(model_results)

    panel_result = ExpertPanelResult(
        stock_code=stock_code,
        stock_name=stock_name,
        models_used=model_names,
        model_results=model_results,
        consensus_score=consensus["score"],
        consensus_advice=consensus["advice"],
        consensus_summary=consensus["summary"],
        consensus_strategy=consensus["strategy"],
    )

    logger.info(f"[ä¸“å®¶ä¼šè¯Š] åˆ†æå®Œæˆ: {consensus['summary']}")
    return panel_result
